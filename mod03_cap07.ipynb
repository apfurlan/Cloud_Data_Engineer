{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P0zINst1da8r"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Instal Java\n",
        "apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install PySpark\n",
        "pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "W_E2MpEQG1jd"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "unset SPARK_HOME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TuQfOFCOdiwq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ[\"HADOOP_OPTS\"] = \"-Djava.library.path=$HADOOP_HOME/lib/native\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/bin/spark-2.3.0-bin-hadoop2.7\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mciZ37Q81SXG"
      },
      "source": [
        "# Class 01 - **Introduction to execution mode**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi_Qip5S2_gw"
      },
      "source": [
        "- **Cluster** : In this mode, the master node receives the Spark code provided by the user and instantiates the processes on each worker node in order to generate drivers and executors for the Spark architecture. The available managers are : Standalone, YARN and Apache Mesos.\n",
        "\n",
        "- **Client** : This mode is very similar to the cluster node, however, the Spark driver is in the machine in which the aplication was sent, i.e., the cluster manager is responsible for maintaining the process in the executors, while the client machine machine stores the drive. \n",
        "\n",
        "- **Local** : In this mode, the Spark application is executed on a single machine, and the paralelizing process occurs between threads. This mode is suggested for studies and tests, due to the ease of working. This mode is not suggested to the production environments. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "932C1R4JtrqR"
      },
      "source": [
        "# Class 04 - **Hans-on**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkOMispH2-CF",
        "outputId": "14cca966-89ab-48e3-aab8-e008905009dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/bin/spark-submit: line 27: /usr/local/bin/spark-2.3.0-bin-hadoop2.7/bin/spark-class: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%%bash \n",
        "\n",
        "spark-submit --py-files /content/drive/MyDrive/igti_bootcamps/eng_dados_cloud/mod3/imdb_cleanning.py, /content/drive/MyDrive/igti_bootcamps/eng_dados_cloud/mod3/variables.py /content/drive/MyDrive/igti_bootcamps/eng_dados_cloud/mod3/spark_app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have several scripts try to pass the scripts as a .zip file"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mod03_cap07.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
