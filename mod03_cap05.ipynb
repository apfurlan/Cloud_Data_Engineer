{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mod03_cap05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0KXQLcURrNx0"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Instal Java\n",
        "apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install PySpark\n",
        "pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "j6wt22Z6ydmy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import abspath\n",
        "warehouse_location = abspath('../content/drive/MyDrive/igti_bootcamps/eng_dados_cloud/mod3/spark-warehouse')"
      ],
      "metadata": {
        "id": "8hh1oFLHyIBA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .config(\"spark.sql.warehouse.dir\",warehouse_location)\n",
        "    .enableHiveSupport()\n",
        "    .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "id": "Ks7mPJsZx3dl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class 01 - **Introduction to Spark-SQL**"
      ],
      "metadata": {
        "id": "zbknfyFfrWxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview of Spark-SQL**\n",
        "\n",
        "Spark SQL is a mechanism of manipulation of data on Spark that uses a SQL language. \n",
        "\n",
        "- Performing ANSI-SQL and HiveQL queries that act directly on the RDDs.\n",
        "- Creating databases, tables and views, similar to a relational database.\n",
        "- Data management using the Spark metadata catalog and its integration with the Hive metastore. \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "Spark is designed to work as online analytical processing (OLAP), not as online transaction processing (OLTP). In other words Spark does not exhibit functions like, insert, alter, delete data in a Database."
      ],
      "metadata": {
        "id": "R021DBVXseAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class 02 - **Databases and Datacatalog**"
      ],
      "metadata": {
        "id": "DtQPlK-Ly4T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as f"
      ],
      "metadata": {
        "id": "BtZRdX_5yobT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**"
      ],
      "metadata": {
        "id": "5D5zuckazlKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "EHczSvVHzifs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_path = '/content/drive/MyDrive/igti_bootcamps/eng_dados_cloud/mod3/title_basics.csv'"
      ],
      "metadata": {
        "id": "ULI2zcZmzxlN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options_dict = {\n",
        "    'sep' : '\\t' , \n",
        "    'header' : 'True'\n",
        "}\n",
        "\n",
        "df_titles = (\n",
        "      spark.read\n",
        "    .format('csv')\n",
        "    .options(**options_dict)  \n",
        "    .load(imdb_path)\n",
        ")\n",
        "\n",
        "df_titles.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po5dSVfoz9C3",
        "outputId": "8760e09f-f761-4732-8b77-362e26cfc9c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
            "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
            "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|     \\N|             1|   Documentary,Short|\n",
            "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|     \\N|             5|     Animation,Short|\n",
            "|tt0000003|    short|      Pauvre Pierrot|      Pauvre Pierrot|      0|     1892|     \\N|             4|Animation,Comedy,...|\n",
            "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|     \\N|            12|     Animation,Short|\n",
            "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|     \\N|             1|        Comedy,Short|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Databases and Catalog**"
      ],
      "metadata": {
        "id": "kN2lqEtg0vtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metadata catalog can be accessed by the object\n",
        "\n",
        "```SparkSession.catalog```\n",
        "\n",
        "The main functionalities are :\n",
        "\n",
        "- ```listDatabases()``` : list all available databases;\n",
        "- ```listTables()``` : list all available tables in a certain database;\n",
        "- ```listFunctions()``` : list available functions in a certain database;\n",
        "- ```refreshTable()``` : update the metadata of a given table;\n",
        "- ```uncacheTable()``` : removes a table saved in memory; \n",
        "- ```clearCache()``` : removes all tables saved in memory;"
      ],
      "metadata": {
        "id": "PO7X_Y_t08XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JrGjfVy06hH",
        "outputId": "6678372c-aaaa-47da-c66b-546f87008a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.catalog.Catalog at 0x7f5e21832dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.clearCache()"
      ],
      "metadata": {
        "id": "mcihno5t3wAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.listDatabases()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rk65Ld40t_1",
        "outputId": "6d56dccf-ee9e-4076-f7a0-ffb57de40d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', description='default database', locationUri='file:/content/spark-warehouse')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.listFunctions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFXNAtYp30py",
        "outputId": "6b6043a4-61b9-4c41-b74f-d9b61ada25ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
              " Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
              " Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
              " Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
              " Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
              " Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
              " Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
              " Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
              " Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
              " Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
              " Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
              " Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
              " Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
              " Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
              " Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
              " Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
              " Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
              " Function(name='acosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Acosh', isTemporary=True),\n",
              " Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
              " Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
              " Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
              " Function(name='any', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
              " Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
              " Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
              " Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
              " Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
              " Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
              " Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
              " Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
              " Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
              " Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
              " Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
              " Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
              " Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
              " Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
              " Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
              " Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
              " Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
              " Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
              " Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
              " Function(name='asinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Asinh', isTemporary=True),\n",
              " Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
              " Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
              " Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
              " Function(name='atanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Atanh', isTemporary=True),\n",
              " Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
              " Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
              " Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
              " Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bit_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg', isTemporary=True),\n",
              " Function(name='bit_count', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseCount', isTemporary=True),\n",
              " Function(name='bit_get', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True),\n",
              " Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
              " Function(name='bit_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg', isTemporary=True),\n",
              " Function(name='bit_xor', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg', isTemporary=True),\n",
              " Function(name='bool_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
              " Function(name='bool_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
              " Function(name='btrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimBoth', isTemporary=True),\n",
              " Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
              " Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
              " Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
              " Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
              " Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
              " Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
              " Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
              " Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
              " Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
              " Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
              " Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
              " Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
              " Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
              " Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
              " Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
              " Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
              " Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
              " Function(name='count_if', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountIf', isTemporary=True),\n",
              " Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
              " Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
              " Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
              " Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
              " Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
              " Function(name='current_catalog', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentCatalog', isTemporary=True),\n",
              " Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
              " Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
              " Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
              " Function(name='current_timezone', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimeZone', isTemporary=True),\n",
              " Function(name='current_user', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentUser', isTemporary=True),\n",
              " Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
              " Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
              " Function(name='date_from_unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFromUnixDate', isTemporary=True),\n",
              " Function(name='date_part', description=None, className='org.apache.spark.sql.catalyst.expressions.DatePart', isTemporary=True),\n",
              " Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
              " Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
              " Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
              " Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
              " Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
              " Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
              " Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
              " Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
              " Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
              " Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
              " Function(name='div', description=None, className='org.apache.spark.sql.catalyst.expressions.IntegralDivide', isTemporary=True),\n",
              " Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
              " Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
              " Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
              " Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
              " Function(name='every', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
              " Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
              " Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
              " Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
              " Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
              " Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
              " Function(name='extract', description=None, className='org.apache.spark.sql.catalyst.expressions.Extract', isTemporary=True),\n",
              " Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
              " Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
              " Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
              " Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
              " Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
              " Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
              " Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True),\n",
              " Function(name='forall', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayForAll', isTemporary=True),\n",
              " Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
              " Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
              " Function(name='from_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.CsvToStructs', isTemporary=True),\n",
              " Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
              " Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
              " Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
              " Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
              " Function(name='getbit', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True),\n",
              " Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
              " Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
              " Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
              " Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
              " Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
              " Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
              " Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
              " Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
              " Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True),\n",
              " Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
              " Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
              " Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
              " Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
              " Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
              " Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
              " Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
              " Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
              " Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
              " Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
              " Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
              " Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
              " Function(name='json_array_length', description=None, className='org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray', isTemporary=True),\n",
              " Function(name='json_object_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonObjectKeys', isTemporary=True),\n",
              " Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
              " Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
              " Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
              " Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
              " Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
              " Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
              " Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
              " Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
              " Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
              " Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
              " Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
              " Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
              " Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
              " Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
              " Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
              " Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
              " Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
              " Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
              " Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
              " Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True),\n",
              " Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
              " Function(name='make_date', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDate', isTemporary=True),\n",
              " Function(name='make_dt_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDTInterval', isTemporary=True),\n",
              " Function(name='make_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeInterval', isTemporary=True),\n",
              " Function(name='make_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeTimestamp', isTemporary=True),\n",
              " Function(name='make_ym_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeYMInterval', isTemporary=True),\n",
              " Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
              " Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
              " Function(name='map_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapEntries', isTemporary=True),\n",
              " Function(name='map_filter', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFilter', isTemporary=True),\n",
              " Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
              " Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
              " Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
              " Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
              " Function(name='map_zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.MapZipWith', isTemporary=True),\n",
              " Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
              " Function(name='max_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy', isTemporary=True),\n",
              " Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
              " Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
              " Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
              " Function(name='min_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MinBy', isTemporary=True),\n",
              " Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
              " Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
              " Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
              " Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
              " Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
              " Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
              " Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
              " Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
              " Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
              " Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
              " Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.Now', isTemporary=True),\n",
              " Function(name='nth_value', description=None, className='org.apache.spark.sql.catalyst.expressions.NthValue', isTemporary=True),\n",
              " Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
              " Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
              " Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
              " Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
              " Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
              " Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
              " Function(name='overlay', description=None, className='org.apache.spark.sql.catalyst.expressions.Overlay', isTemporary=True),\n",
              " Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
              " Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
              " Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
              " Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
              " Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
              " Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
              " Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
              " Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
              " Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
              " Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
              " Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
              " Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
              " Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
              " Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
              " Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
              " Function(name='raise_error', description=None, className='org.apache.spark.sql.catalyst.expressions.RaiseError', isTemporary=True),\n",
              " Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
              " Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
              " Function(name='random', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
              " Function(name='range', description=None, className='org.apache.spark.sql.catalyst.plans.logical.Range', isTemporary=True),\n",
              " Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
              " Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
              " Function(name='regexp', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
              " Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
              " Function(name='regexp_extract_all', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtractAll', isTemporary=True),\n",
              " Function(name='regexp_like', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
              " Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
              " Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
              " Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
              " Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
              " Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
              " Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
              " Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
              " Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
              " Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
              " Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True),\n",
              " Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
              " Function(name='schema_of_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfCsv', isTemporary=True),\n",
              " Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
              " Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
              " Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
              " Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
              " Function(name='session_window', description=None, className='org.apache.spark.sql.catalyst.expressions.SessionWindow', isTemporary=True),\n",
              " Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
              " Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
              " Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
              " Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
              " Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
              " Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
              " Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
              " Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
              " Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
              " Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
              " Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
              " Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
              " Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
              " Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
              " Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='some', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
              " Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
              " Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
              " Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
              " Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
              " Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
              " Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
              " Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
              " Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
              " Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
              " Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
              " Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
              " Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
              " Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
              " Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
              " Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
              " Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='timestamp_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp', isTemporary=True),\n",
              " Function(name='timestamp_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.MillisToTimestamp', isTemporary=True),\n",
              " Function(name='timestamp_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp', isTemporary=True),\n",
              " Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='to_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToCsv', isTemporary=True),\n",
              " Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
              " Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
              " Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
              " Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
              " Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
              " Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
              " Function(name='transform_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformKeys', isTemporary=True),\n",
              " Function(name='transform_values', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformValues', isTemporary=True),\n",
              " Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
              " Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
              " Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
              " Function(name='try_add', description=None, className='org.apache.spark.sql.catalyst.expressions.TryAdd', isTemporary=True),\n",
              " Function(name='try_divide', description=None, className='org.apache.spark.sql.catalyst.expressions.TryDivide', isTemporary=True),\n",
              " Function(name='typeof', description=None, className='org.apache.spark.sql.catalyst.expressions.TypeOf', isTemporary=True),\n",
              " Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
              " Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
              " Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
              " Function(name='unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixDate', isTemporary=True),\n",
              " Function(name='unix_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMicros', isTemporary=True),\n",
              " Function(name='unix_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMillis', isTemporary=True),\n",
              " Function(name='unix_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixSeconds', isTemporary=True),\n",
              " Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
              " Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
              " Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
              " Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
              " Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
              " Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
              " Function(name='version', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkVersion', isTemporary=True),\n",
              " Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
              " Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
              " Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
              " Function(name='width_bucket', description=None, className='org.apache.spark.sql.catalyst.expressions.WidthBucket', isTemporary=True),\n",
              " Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
              " Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
              " Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
              " Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
              " Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
              " Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
              " Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
              " Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
              " Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
              " Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
              " Function(name='xxhash64', description=None, className='org.apache.spark.sql.catalyst.expressions.XxHash64', isTemporary=True),\n",
              " Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
              " Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
              " Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
              " Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.listTables()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rEri_qS4DgT",
        "outputId": "cb4aa9bf-8250-4a6d-ca05-b7dbc2255352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark databases are a tool to organize tables. They can be seen as something very close to server databases in relataional databases. Spark uses a default database, which is used to create tables, views and perform queries. The structures persist in different sections: i user change the database, all tabels will remain in the previous database and will need to be required differently. \n",
        "\n",
        "Some SQL command to work with databases: \n",
        "\n",
        "- `SHOW DATABASES` : list all databases\n",
        "- `CREATE DATABASE <db_name>` : create database\n",
        "- `USE <db_name>`: define the database as current database for performing queries.\n",
        "  - **OBS:** if you change database, is possible to access the previous database by using the prefix 'db_name'. For example :\n",
        "\n",
        "  > `USE db2` \\\n",
        "  > `SELECT * FROM db1.table` \n",
        "\n",
        "- `SELECT current_database()` : returns current database\n",
        "- `DROP DATABASE IF EXISTS <db_name>` : delete database among those that have been defined. "
      ],
      "metadata": {
        "id": "sXpTJUWq46IN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class 03 - Creating Tables and Views"
      ],
      "metadata": {
        "id": "rSjZcTvytpJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tables\n",
        "\n",
        "- **Managed Tables** : Spark manages data and metadata of table. Instruction like as DROP TABKE affect the data written in disk.\n",
        "- **Unmanaged Tables** : Spark manages only metadata. The data written in disk can not be altered.  "
      ],
      "metadata": {
        "id": "A45tUc7Ruxj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_titles_sample = df_titles.sample(fraction=0.1)"
      ],
      "metadata": {
        "id": "4MwsXp8CvuLE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_titles_sample.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--2Gc78kzAyo",
        "outputId": "2e820209-8ecb-4e7d-f5d6-2ac29b78b706"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|           genres|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|     \\N|            12|  Animation,Short|\n",
            "|tt0000006|    short|   Chinese Opium Den|   Chinese Opium Den|      0|     1894|     \\N|             1|            Short|\n",
            "|tt0000017|    short|Italienischer Bau...|Italienischer Bau...|      0|     1895|     \\N|             1|Documentary,Short|\n",
            "|tt0000024|    short|Opening of the Ki...|Opening of the Ki...|      0|     1895|     \\N|            \\N|       News,Short|\n",
            "|tt0000032|    short|Die Serpentintnz...|Die Serpentintnz...|      0|     1895|     \\N|             1|            Short|\n",
            "|tt0000035|    short|Watering the Flowers|          L'arroseur|      0|     1896|     \\N|             1|     Comedy,Short|\n",
            "|tt0000050|    short|   Bb et fillettes|   Bb et fillettes|      0|     1896|     \\N|            \\N|Documentary,Short|\n",
            "|tt0000051|    short|The Bohemian Enca...|Campement de boh...|      0|     1896|     \\N|            \\N|Documentary,Short|\n",
            "|tt0000068|    short|  Unloading the Boat|Dchargement de b...|      0|     1896|     \\N|            \\N|Documentary,Short|\n",
            "|tt0000072|    short|Officers of Frenc...|Dpart des officiers|      0|     1896|     \\N|            \\N|            Short|\n",
            "|tt0000076|    short|Exit of Rip and t...|Exit of Rip and t...|      0|     1896|     \\N|             1|      Drama,Short|\n",
            "|tt0000103|    short|Smarter than the ...|Plus fort que le ...|      0|     1896|     \\N|            \\N|            Short|\n",
            "|tt0000108|    short|Rip Leaving Sleep...|Rip Leaving Sleep...|      0|     1896|     \\N|             1|      Drama,Short|\n",
            "|tt0000113|    short|Rip's Twenty Year...|Rip's Twenty Year...|      0|     1896|     \\N|             0|      Drama,Short|\n",
            "|tt0000117|    short|Sada do Pessoal ...|Sada do Pessoal ...|      0|     1896|     \\N|             1|Documentary,Short|\n",
            "|tt0000123|    short|           Conjuring|Sance de prestid...|      0|     1896|     \\N|            \\N|            Short|\n",
            "|tt0000142|    short|Buffalo Bill and ...|Buffalo Bill and ...|      0|     1897|     \\N|            \\N|       News,Short|\n",
            "|tt0000150|    short|Danse fleur de lotus|Danse fleur de lotus|      0|     1897|     \\N|             1|Documentary,Short|\n",
            "|tt0000161|    short|      Leon de danse|      Leon de danse|      0|     1897|     \\N|            \\N|     Comedy,Short|\n",
            "|tt0000204|    short| Jsus devant Pilate| Jsus devant Pilate|      0|     1898|     \\N|            \\N|            Short|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Managed Tables"
      ],
      "metadata": {
        "id": "f7Y8dlH4wAti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_titles_sample.write.saveAsTable(\"title_basics_managed2\")"
      ],
      "metadata": {
        "id": "9eRky275wFHU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CREATE TABLE title_basics_managed (schema)`"
      ],
      "metadata": {
        "id": "j2zZK6ro0rP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Unmanaged Tables"
      ],
      "metadata": {
        "id": "zP5jrxbGzMXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_titles_sample.write.option('path','../content/drive/MyDrive/igti_bootcamps/eng_dados_cloud/mod3/title_basics_unmanaged').saveAsTable('title_basics_unmanaged')"
      ],
      "metadata": {
        "id": "sZ2AtwfTxvxd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CREATE EXTERNAL TABLE title_basics_unmanaged (schema)` \\\n",
        "`USING parquet OPTIONS (path '../content/drive/MyDrive/igti_bootcamps/eng_dados_cloud/mod3/title_basics_unmanaged')` "
      ],
      "metadata": {
        "id": "ea0jtWPX02-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.listTables()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdkFnlZQ1Rb1",
        "outputId": "2acd81f3-38c6-4f09-e219-a55b7e26fe89"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='title_basics_managed', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='title_basics_unmanaged', database='default', description=None, tableType='EXTERNAL', isTemporary=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View"
      ],
      "metadata": {
        "id": "FDmwSC0J14jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Views"
      ],
      "metadata": {
        "id": "kqDJRgZw2HhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_titles_sample.createOrReplaceTempView('title_basics_view')"
      ],
      "metadata": {
        "id": "EyottRbg2O-9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CREATE OR REPLACE TEMP VIEW AS title_basics_view` \\\n",
        "`SELECT * FROM <table_name>`"
      ],
      "metadata": {
        "id": "Awiw4A0S2Xia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Global Views"
      ],
      "metadata": {
        "id": "S8wVqoJI2qJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_titles_sample.createOrReplaceGlobalTempView('title_basics_global_view')"
      ],
      "metadata": {
        "id": "wGXVPIEk2WyV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CREATE OR REPLACE GLOBAL TEMP VIEW AS title_basics_view` \\\n",
        "`SELECT * FROM <table_name>`"
      ],
      "metadata": {
        "id": "eT89_zJJ218_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.listTables()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByPVcL4H2zOa",
        "outputId": "8c9750ef-6b4b-4a08-9f60-a32c92de2a9b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='title_basics_managed', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='title_basics_unmanaged', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
              " Table(name='title_basics_view', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deleting Views"
      ],
      "metadata": {
        "id": "1Y8v-baC3Y0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.dropTempView('title_basics_view')"
      ],
      "metadata": {
        "id": "yu4gdqR93W2H"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.dropTempView('title_basics_global_view')"
      ],
      "metadata": {
        "id": "arNy5uGe3jtJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.listTables()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYDbEFg-3oHw",
        "outputId": "beaede8b-6eec-4516-ba31-a05aab9a1abf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='title_basics_managed', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='title_basics_unmanaged', database='default', description=None, tableType='EXTERNAL', isTemporary=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class 04 - Queries in Spark-SQL"
      ],
      "metadata": {
        "id": "PNq6TTw14tJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SHOW DATABASES\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtQqti3o41XR",
        "outputId": "b27abc1d-009e-4e77-c1ea-244d705ba052"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SHOW TABLES').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xRu3Hy37f9b",
        "outputId": "99eb8ad1-0d2c-434a-899c-df22dc8963a6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+-----------+\n",
            "|namespace|           tableName|isTemporary|\n",
            "+---------+--------------------+-----------+\n",
            "|  default|title_basics_managed|      false|\n",
            "|  default|title_basics_mana...|      false|\n",
            "|  default|title_basics_unma...|      false|\n",
            "+---------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SELECT * FROM default.title_basics_managed LIMIT 10').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2dJsOiR8HGr",
        "outputId": "0e9ce274-ae00-449f-ccd6-cd0fc2093038"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|           genres|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|     \\N|             5|  Animation,Short|\n",
            "|tt0000008|    short|Edison Kinetoscop...|Edison Kinetoscop...|      0|     1894|     \\N|             1|Documentary,Short|\n",
            "|tt0000017|    short|Italienischer Bau...|Italienischer Bau...|      0|     1895|     \\N|             1|Documentary,Short|\n",
            "|tt0000025|    short|The Oxford and Ca...|The Oxford and Ca...|      0|     1895|     \\N|            \\N| News,Short,Sport|\n",
            "|tt0000027|    short|Cordeliers' Squar...|Place des Cordeli...|      0|     1895|     \\N|             1|Documentary,Short|\n",
            "|tt0000046|    short|    Bois de Boulogne|    Bois de Boulogne|      0|     1896|     \\N|            \\N|Documentary,Short|\n",
            "|tt0000054|    short|    A Merry-Go-Round| Les chevaux de bois|      0|     1896|     \\N|            \\N|            Short|\n",
            "|tt0000058|    short|Cortge de tzar a...|Cortge de tzar a...|      0|     1896|     \\N|            \\N|Documentary,Short|\n",
            "|tt0000062|    short|    Danse serpentine|    Danse serpentine|      0|     1896|     \\N|             1|            Short|\n",
            "|tt0000078|    short|Feira de Gado na ...|Feira de Gado na ...|      0|     1896|     \\N|             1|Documentary,Short|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "CREATE TABLE tab1\n",
        "(tconst STRING,\n",
        "titleType STRING,\n",
        "primaryTitle STRING, \n",
        "originalTitle STRING, \n",
        "isAdult STRING, \n",
        "startYear STRING, \n",
        "endYear STRING, \n",
        "runtimeMinutes STRING, \n",
        "genres STRING)\n",
        "  \"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "QpMXlOS18uVm",
        "outputId": "0e00f48c-5d48-4156-af26-f38c18a04204"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 2))\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-381486717c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mruntimeMinutes\u001b[0m \u001b[0mSTRING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m genres STRING)\n\u001b[0;32m---> 12\u001b[0;31m   \"\"\").show()\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `default`.`tab1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SELECT * FROM default.title_basics_managed LIMIT 10').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vwmk96f9ed0",
        "outputId": "42d95b7e-e1c4-4ae6-f64b-2bc73742600b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|           genres|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|     \\N|             5|  Animation,Short|\n",
            "|tt0000008|    short|Edison Kinetoscop...|Edison Kinetoscop...|      0|     1894|     \\N|             1|Documentary,Short|\n",
            "|tt0000017|    short|Italienischer Bau...|Italienischer Bau...|      0|     1895|     \\N|             1|Documentary,Short|\n",
            "|tt0000025|    short|The Oxford and Ca...|The Oxford and Ca...|      0|     1895|     \\N|            \\N| News,Short,Sport|\n",
            "|tt0000027|    short|Cordeliers' Squar...|Place des Cordeli...|      0|     1895|     \\N|             1|Documentary,Short|\n",
            "|tt0000046|    short|    Bois de Boulogne|    Bois de Boulogne|      0|     1896|     \\N|            \\N|Documentary,Short|\n",
            "|tt0000054|    short|    A Merry-Go-Round| Les chevaux de bois|      0|     1896|     \\N|            \\N|            Short|\n",
            "|tt0000058|    short|Cortge de tzar a...|Cortge de tzar a...|      0|     1896|     \\N|            \\N|Documentary,Short|\n",
            "|tt0000062|    short|    Danse serpentine|    Danse serpentine|      0|     1896|     \\N|             1|            Short|\n",
            "|tt0000078|    short|Feira de Gado na ...|Feira de Gado na ...|      0|     1896|     \\N|             1|Documentary,Short|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('DROP TABLE title_basics_managed').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ7lPu7S_LrR",
        "outputId": "847a1a1c-4601-4b2e-d1af-3a94b62e1e54"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SHOW TABLES').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CERkxt1c_Ud_",
        "outputId": "26df0ecd-62d9-4396-c773-f1349e4fdf09"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+-----------+\n",
            "|namespace|           tableName|isTemporary|\n",
            "+---------+--------------------+-----------+\n",
            "|  default|title_basics_mana...|      false|\n",
            "|  default|title_basics_unma...|      false|\n",
            "+---------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('SHOW VIEWS').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov82-CGKAVto",
        "outputId": "33dcbe21-8096-49d5-f648-84fd324e6a64"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------------+-----------+\n",
            "|namespace|         viewName|isTemporary|\n",
            "+---------+-----------------+-----------+\n",
            "|         |title_basics_view|       true|\n",
            "+---------+-----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_titles_sample.createOrReplaceTempView('title_basics_view')\n",
        "spark.sql('SELECT * FROM title_basics_view LIMIT 10').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Enp6KAvADTz",
        "outputId": "19bc1ab9-44a0-41ee-9bf4-544164026ddd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|           genres|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "|tt0000008|    short|Edison Kinetoscop...|Edison Kinetoscop...|      0|     1894|     \\N|             1|Documentary,Short|\n",
            "|tt0000029|    short|         Baby's Meal|       Repas de bb|      0|     1895|     \\N|             1|Documentary,Short|\n",
            "|tt0000030|    short|  Rough Sea at Dover|  Rough Sea at Dover|      0|     1895|     \\N|             1|Documentary,Short|\n",
            "|tt0000043|    short|   Batteuse  vapeur|   Batteuse  vapeur|      0|     1896|     \\N|            \\N|Documentary,Short|\n",
            "|tt0000044|    short|          Le bivouac|          Le bivouac|      0|     1896|     \\N|             1|            Short|\n",
            "|tt0000049|    short|Boxing Match; or,...|Boxing Match; or,...|      0|     1896|     \\N|            \\N|      Short,Sport|\n",
            "|tt0000053|    short|A Chegada do Comb...|A Chegada do Comb...|      0|     1897|     \\N|            \\N|Documentary,Short|\n",
            "|tt0000063|    short| Dessinateur express| Dessinateur express|      0|     1896|     \\N|            \\N|            Short|\n",
            "|tt0000066|    short|Dessinateur: Von ...|Dessinateur: Von ...|      0|     1896|     \\N|            \\N|            Short|\n",
            "|tt0000069|    short|       Post No Bills|  Dfense d'afficher|      0|     1896|     \\N|             1|            Short|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT CAST(runtimeMinutes as INT) FROM title_basics_view\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TakLYXwjA6yK",
        "outputId": "4564f9f3-d90f-46b0-8922-804c58c8949c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|runtimeMinutes|\n",
            "+--------------+\n",
            "|             1|\n",
            "|             1|\n",
            "|             1|\n",
            "|          null|\n",
            "|             1|\n",
            "+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT CAST(runtimeMinutes as INT) FROM title_basics_view\")\\\n",
        ".withColumn('sum(Min)', f.col('runtimeMinutes') + 1).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kb5R4nEBU4W",
        "outputId": "b768e13c-9f5f-4ada-faf7-f8f32cdec9ed"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------+\n",
            "|runtimeMinutes|sum(Min)|\n",
            "+--------------+--------+\n",
            "|             1|       2|\n",
            "|             1|       2|\n",
            "|             1|       2|\n",
            "|          null|    null|\n",
            "|             1|       2|\n",
            "+--------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}